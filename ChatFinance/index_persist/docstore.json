<<<<<<< HEAD
{}
=======
{"docstore/metadata": {"d33a0a17-270a-4300-b46f-7c4f5d5494d6": {"doc_hash": "dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758e8014a24c7fa039e91"}, "3e8f08c8-d325-4cd3-98af-def609028c33": {"doc_hash": "2a88543a2946fa65e01413baef2e6629524783ec1c05c35a2dab1ce1c549804a"}, "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b": {"doc_hash": "aeb916b6ca364507fcd7e06735f0c1aa48d1ae73f2aa5b5c351e66d48727d3f4"}, "2382747a-4f32-40bd-a03b-994baad7fb53": {"doc_hash": "eef1acbb6acd03ecabe292edca3a313aeacfcf1110520520116262a69901456b"}, "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e": {"doc_hash": "5b93d75d9dc2a2f8e65850683476734d426911c84c09aeef348aa166c0e8a9eb"}, "269f2882-3aa7-4715-be8d-06056c891a66": {"doc_hash": "2a88543a2946fa65e01413baef2e6629524783ec1c05c35a2dab1ce1c549804a", "ref_doc_id": "3e8f08c8-d325-4cd3-98af-def609028c33"}, "9316a362-0582-420c-bec2-8be58c2f5ed3": {"doc_hash": "baba233ecf4f8a553cc7faa4442ceb07d29da82f7d7dccafa32ec688002b6728", "ref_doc_id": "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b"}, "dac44e27-ab23-4312-9e2f-c8c4adbffa6c": {"doc_hash": "f7faa5ea335f54654e77c38fa938dd3b1ae226e017057f48ecf0cd6484430efe", "ref_doc_id": "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b"}, "10cde64c-90c6-4d1c-919b-7b5bc612e09e": {"doc_hash": "6d9981cef851436cd1715f413227eb0ee84475ef6eed6fafde17f07f3bad310d", "ref_doc_id": "2382747a-4f32-40bd-a03b-994baad7fb53"}, "5d340179-ec56-4fd9-9c4c-328356523c31": {"doc_hash": "3aee643eaac02cb6ac06a5d9c4f19f3d43cbb738785bcc3775ffa4c05a126755", "ref_doc_id": "2382747a-4f32-40bd-a03b-994baad7fb53"}, "703d8ac3-b766-4d76-b1b0-e7b7fd88d513": {"doc_hash": "a32c49ae8bbb453337f12b8b672420fc50b860bb317ae8a47df44bc45d6e4ceb", "ref_doc_id": "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e"}, "4ed178f5-5f6a-4d2f-899e-99bfcaa96b7e": {"doc_hash": "fb545fb316846ea5137cbb0e58f433c6331d8985a59134021cefc283aa2eb6a7", "ref_doc_id": "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e"}}, "docstore/data": {"269f2882-3aa7-4715-be8d-06056c891a66": {"__data__": {"text": "# Define here the models for your scraped items\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass NewsCrawlerItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    url = scrapy.Field()\n    body = scrapy.Field()\n", "doc_id": "269f2882-3aa7-4715-be8d-06056c891a66", "embedding": null, "doc_hash": "2a88543a2946fa65e01413baef2e6629524783ec1c05c35a2dab1ce1c549804a", "extra_info": null, "node_info": {"start": 0, "end": 309, "_node_type": "1"}, "relationships": {"1": "3e8f08c8-d325-4cd3-98af-def609028c33"}}, "__type__": "1"}, "9316a362-0582-420c-bec2-8be58c2f5ed3": {"__data__": {"text": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass NewsCrawlerSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u2019t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info(\"Spider opened: %s\" % spider.name)\n\n\nclass NewsCrawlerDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n     ", "doc_id": "9316a362-0582-420c-bec2-8be58c2f5ed3", "embedding": null, "doc_hash": "baba233ecf4f8a553cc7faa4442ceb07d29da82f7d7dccafa32ec688002b6728", "extra_info": null, "node_info": {"start": 0, "end": 2773, "_node_type": "1"}, "relationships": {"1": "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b", "3": "dac44e27-ab23-4312-9e2f-c8c4adbffa6c"}}, "__type__": "1"}, "dac44e27-ab23-4312-9e2f-c8c4adbffa6c": {"__data__": {"text": "or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info(\"Spider opened: %s\" % spider.name)\n", "doc_id": "dac44e27-ab23-4312-9e2f-c8c4adbffa6c", "embedding": null, "doc_hash": "f7faa5ea335f54654e77c38fa938dd3b1ae226e017057f48ecf0cd6484430efe", "extra_info": null, "node_info": {"start": 2713, "end": 3656, "_node_type": "1"}, "relationships": {"1": "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b", "2": "9316a362-0582-420c-bec2-8be58c2f5ed3"}}, "__type__": "1"}, "10cde64c-90c6-4d1c-919b-7b5bc612e09e": {"__data__": {"text": "# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n\n\n# useful for handling different item types with a single interface\nfrom itemadapter import ItemAdapter\nfrom scrapy.exceptions import DropItem\n\nimport datetime\nimport os\nfrom pathlib import Path\nimport pymongo\nimport uuid\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\nclass NewsCrawlerPipeline:\n    @classmethod\n    def from_crawler(cls, crawler):\n        cls.mongodb_uri = crawler.settings.get(\"MONGODB_URI\", \"mongodb://localhost:27017/\")\n        cls.db_name = crawler.settings.get(\"MONGODB_DATABASE\", \"financial_news\")\n        cls.user = crawler.settings.get(\"MONGODB_USER\")\n        cls.password = crawler.settings.get(\"MONGODB_PASSWORD\")\n        cls.collection_name = crawler.settings.get(\"MONGODB_COLLECTION\", \"news_crawl_status\")\n        cls.file_dir = os.getenv(\"NEWS_DATA_DIR\")\n        if not cls.file_dir:\n            raise ValueError(\"File directory path not found in .env file. Please define NEWS_DATA_DIR.\")\n        return cls()\n    \n\n    def open_spider(self, spider):\n        # Setup connection with MongoDB\n        self.client = pymongo.MongoClient(self.mongodb_uri)\n        self.db = self.client[self.db_name]\n        self.collection = self.db[self.collection_name]\n\n    def process_item(self, item, spider):\n        # Check duplication\n        url = item['url']\n        query = {\"url\": url}\n        doc = self.collection.find_one(query)\n        if doc:\n            raise DropItem(f\"Already crawled {item['url']}.\")\n        # Write data to html.\n        doc_uuid = str(uuid.uuid4())\n        doc_path = os.path.join(self.file_dir, f\"{doc_uuid}.html\")\n        try:\n            Path(doc_path).write_bytes(item['body'])\n        except:\n            raise DropItem(f\"Write doc failed {item['url']}.\")\n        # Write status to MongoDB\n        current_time = datetime.datetime.utcnow()\n        update_doc = {\n            \"url\": url,\n            \"doc_name\": doc_uuid, \n            \"last_modified\": current_time, \n            \"created_at\": current_time, \n            \"indexed\": False\n        }\n        insert_result = self.collection.insert_one(update_doc)\n        if insert_result.inserted_id:\n           ", "doc_id": "10cde64c-90c6-4d1c-919b-7b5bc612e09e", "embedding": null, "doc_hash": "6d9981cef851436cd1715f413227eb0ee84475ef6eed6fafde17f07f3bad310d", "extra_info": null, "node_info": {"start": 0, "end": 2327, "_node_type": "1"}, "relationships": {"1": "2382747a-4f32-40bd-a03b-994baad7fb53", "3": "5d340179-ec56-4fd9-9c4c-328356523c31"}}, "__type__": "1"}, "5d340179-ec56-4fd9-9c4c-328356523c31": {"__data__": {"text": "           spider.log(\"success!\")\n        else:\n            raise DropItem(f\"Insert status failed {item['url']}\")\n        return item\n    \n    def close_spider(self, spider):\n        # Close connection when spider completed.\n        self.client.close()\n", "doc_id": "5d340179-ec56-4fd9-9c4c-328356523c31", "embedding": null, "doc_hash": "3aee643eaac02cb6ac06a5d9c4f19f3d43cbb738785bcc3775ffa4c05a126755", "extra_info": null, "node_info": {"start": 2317, "end": 2570, "_node_type": "1"}, "relationships": {"1": "2382747a-4f32-40bd-a03b-994baad7fb53", "2": "10cde64c-90c6-4d1c-919b-7b5bc612e09e"}}, "__type__": "1"}, "703d8ac3-b766-4d76-b1b0-e7b7fd88d513": {"__data__": {"text": "# Scrapy settings for news_crawler project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\nBOT_NAME = \"news_crawler\"\n\nSPIDER_MODULES = [\"news_crawler.spiders\"]\nNEWSPIDER_MODULE = \"news_crawler.spiders\"\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = \"news_crawler (+http://www.yourdomain.com)\"\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n#    \"Accept-Language\": \"en\",\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    \"news_crawler.middlewares.NewsCrawlerSpiderMiddleware\": 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    \"news_crawler.middlewares.NewsCrawlerDownloaderMiddleware\": 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n    \"news_crawler.pipelines.NewsCrawlerPipeline\": 300,\n}\n# MongoDB connection\nMONGODB_URI = \"mongodb://localhost:27017/\"\nMONGODB_DATABASE = \"financialnews\"\nMONGODB_USER = \"newsCrawler\"\nMONGODB_PASSWORD = \"crawlnews\"\nMONGODB_COLLECTION = \"news_crawl_status\"\n\n# Update to use environment variable for file directory\nFILE_DIR = os.getenv(\"NEWS_DATA_DIR\")\nif not FILE_DIR:\n    raise ValueError(\"File directory path not found in .env file. Please define NEWS_DATA_DIR.\")\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See", "doc_id": "703d8ac3-b766-4d76-b1b0-e7b7fd88d513", "embedding": null, "doc_hash": "a32c49ae8bbb453337f12b8b672420fc50b860bb317ae8a47df44bc45d6e4ceb", "extra_info": null, "node_info": {"start": 0, "end": 2803, "_node_type": "1"}, "relationships": {"1": "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e", "3": "4ed178f5-5f6a-4d2f-899e-99bfcaa96b7e"}}, "__type__": "1"}, "4ed178f5-5f6a-4d2f-899e-99bfcaa96b7e": {"__data__": {"text": "Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = \"httpcache\"\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n\n# Set settings whose default value is deprecated to a future-proof value\nREQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\nTWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\nFEED_EXPORT_ENCODING = \"utf-8\"\n\n# Log level\nLOG_LEVEL = 'INFO'\nLOG_FILE = 'log.txt'\n", "doc_id": "4ed178f5-5f6a-4d2f-899e-99bfcaa96b7e", "embedding": null, "doc_hash": "fb545fb316846ea5137cbb0e58f433c6331d8985a59134021cefc283aa2eb6a7", "extra_info": null, "node_info": {"start": 2728, "end": 3904, "_node_type": "1"}, "relationships": {"1": "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e", "2": "703d8ac3-b766-4d76-b1b0-e7b7fd88d513"}}, "__type__": "1"}}, "docstore/ref_doc_info": {"3e8f08c8-d325-4cd3-98af-def609028c33": {"doc_ids": ["269f2882-3aa7-4715-be8d-06056c891a66"], "extra_info": {}}, "a891a93f-d084-40f6-8c38-5ff7c2cb4e0b": {"doc_ids": ["9316a362-0582-420c-bec2-8be58c2f5ed3", "dac44e27-ab23-4312-9e2f-c8c4adbffa6c"], "extra_info": {}}, "2382747a-4f32-40bd-a03b-994baad7fb53": {"doc_ids": ["10cde64c-90c6-4d1c-919b-7b5bc612e09e", "5d340179-ec56-4fd9-9c4c-328356523c31"], "extra_info": {}}, "b3ceb331-3b0c-4de7-ac4b-8b72eedc0f8e": {"doc_ids": ["703d8ac3-b766-4d76-b1b0-e7b7fd88d513", "4ed178f5-5f6a-4d2f-899e-99bfcaa96b7e"], "extra_info": {}}}}
>>>>>>> Mohitha
